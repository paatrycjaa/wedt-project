{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.SemEvalData import SemEvalData\n",
    "from src.JigsawData import JigsawData\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer,  text_to_word_sequence\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed, Dropout, Flatten, SpatialDropout1D\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from src.Attention import Attention\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from src.preprocessing import get_embeddings_index, get_embeddings_matrix, getSpansByToxicWords\n",
    "from keras import Sequential\n",
    "from test_sentence import preprocess_lstm, test_lime, vectorize, Transform, getPredictedWordsFromSentence\n",
    "import pickle\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##set to .env\n",
    "MAX_FEATURES = 200000 # maximum number of unique words that should be included in the tokenized word index\n",
    "MAX_WORD_NUM = 40     # maximum number of letters in sentence?\n",
    "EMBED_SIZE = 50  ## same value as in dimension of glove\n",
    "VAL_SPLIT = 0.2  \n",
    "REG_PARAM = 1e-13\n",
    "l2_reg = regularizers.l2(REG_PARAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/patrycja/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "train_data_semeval = SemEvalData(MAX_WORD_NUM)\n",
    "train_data_semeval.load_data(\"data/tsd_trial.csv\")\n",
    "train_df_preprocessed = train_data_semeval.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxic_words</th>\n",
       "      <th>original_text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>diff</th>\n",
       "      <th>toxicity_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[15, 16, 17, 18, 19, 27, 28, 29, 30, 31]</td>\n",
       "      <td>because hes a moron and a bigot. its not any m...</td>\n",
       "      <td>1</td>\n",
       "      <td>[moron, bigot]</td>\n",
       "      <td>Because he's a moron and a bigot. It's not any...</td>\n",
       "      <td>[because hes a moron and a bigot., its not any...</td>\n",
       "      <td>[10, 36]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[29, 30, 31, 32, 33, 34]</td>\n",
       "      <td>how about we stop protecting idiots and let na...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>How about we stop protecting idiots and let na...</td>\n",
       "      <td>[how about we stop protecting idiots and let n...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[166, 167, 168, 169, 170, 171]</td>\n",
       "      <td>if people  were  smart, they would  boycott th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>If people  were  smart, they would  Boycott th...</td>\n",
       "      <td>[if people  were  smart, they would  boycott t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[87, 88, 89, 90, 91, 92]</td>\n",
       "      <td>trump claimed that russia will never invade th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>Trump Claimed that Russia will never invade th...</td>\n",
       "      <td>[trump claimed that russia will never invade t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>as long as your willing to pay a lot more for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>As long as your willing to pay a lot more for ...</td>\n",
       "      <td>[as long as your willing to pay a lot more for...</td>\n",
       "      <td>[148]</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>[129, 130, 131, 132, 133, 134]</td>\n",
       "      <td>but ... trumps not bluffing. hes prepared to g...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>But ... Trump's not bluffing. He's prepared to...</td>\n",
       "      <td>[but ... trumps not bluffing., hes prepared to...</td>\n",
       "      <td>[13, 32, 151, 159, 166]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>[126, 127, 128, 129, 130, 131]</td>\n",
       "      <td>cant believe the limited knowledge of this art...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>Can't believe the limited knowledge of this Ar...</td>\n",
       "      <td>[cant believe the limited knowledge of this ar...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>[24, 25, 26, 27, 28, 29]</td>\n",
       "      <td>i think it conservative idiots who cannot reac...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>I think it conservative idiots who cannot reac...</td>\n",
       "      <td>[i think it conservative idiots who cannot rea...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>youre an id*ot...go away.</td>\n",
       "      <td>1</td>\n",
       "      <td>[youre an id*ot]</td>\n",
       "      <td>You're an id*ot...Go away.</td>\n",
       "      <td>[youre an id*ot...go away.]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>[136, 137, 138, 139, 140, 141]</td>\n",
       "      <td>unless there is wording in the employment cont...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>Unless there is wording in the employment cont...</td>\n",
       "      <td>[unless there is wording in the employment con...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 spans  \\\n",
       "0             [15, 16, 17, 18, 19, 27, 28, 29, 30, 31]   \n",
       "1                             [29, 30, 31, 32, 33, 34]   \n",
       "2                       [166, 167, 168, 169, 170, 171]   \n",
       "3                             [87, 88, 89, 90, 91, 92]   \n",
       "4                                                   []   \n",
       "..                                                 ...   \n",
       "685                     [129, 130, 131, 132, 133, 134]   \n",
       "686                     [126, 127, 128, 129, 130, 131]   \n",
       "687                           [24, 25, 26, 27, 28, 29]   \n",
       "688  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "689                     [136, 137, 138, 139, 140, 141]   \n",
       "\n",
       "                                                  text  toxicity  \\\n",
       "0    because hes a moron and a bigot. its not any m...         1   \n",
       "1    how about we stop protecting idiots and let na...         1   \n",
       "2    if people  were  smart, they would  boycott th...         1   \n",
       "3    trump claimed that russia will never invade th...         1   \n",
       "4    as long as your willing to pay a lot more for ...         0   \n",
       "..                                                 ...       ...   \n",
       "685  but ... trumps not bluffing. hes prepared to g...         1   \n",
       "686  cant believe the limited knowledge of this art...         1   \n",
       "687  i think it conservative idiots who cannot reac...         1   \n",
       "688                          youre an id*ot...go away.         1   \n",
       "689  unless there is wording in the employment cont...         1   \n",
       "\n",
       "          toxic_words                                      original_text  \\\n",
       "0      [moron, bigot]  Because he's a moron and a bigot. It's not any...   \n",
       "1            [idiots]  How about we stop protecting idiots and let na...   \n",
       "2            [idiots]  If people  were  smart, they would  Boycott th...   \n",
       "3            [stupid]  Trump Claimed that Russia will never invade th...   \n",
       "4                  []  As long as your willing to pay a lot more for ...   \n",
       "..                ...                                                ...   \n",
       "685          [stupid]  But ... Trump's not bluffing. He's prepared to...   \n",
       "686          [stupid]  Can't believe the limited knowledge of this Ar...   \n",
       "687          [idiots]  I think it conservative idiots who cannot reac...   \n",
       "688  [youre an id*ot]                         You're an id*ot...Go away.   \n",
       "689          [stupid]  Unless there is wording in the employment cont...   \n",
       "\n",
       "                                             sentences  \\\n",
       "0    [because hes a moron and a bigot., its not any...   \n",
       "1    [how about we stop protecting idiots and let n...   \n",
       "2    [if people  were  smart, they would  boycott t...   \n",
       "3    [trump claimed that russia will never invade t...   \n",
       "4    [as long as your willing to pay a lot more for...   \n",
       "..                                                 ...   \n",
       "685  [but ... trumps not bluffing., hes prepared to...   \n",
       "686  [cant believe the limited knowledge of this ar...   \n",
       "687  [i think it conservative idiots who cannot rea...   \n",
       "688                        [youre an id*ot...go away.]   \n",
       "689  [unless there is wording in the employment con...   \n",
       "\n",
       "                        diff          toxicity_sentence  \n",
       "0                   [10, 36]                 [1.0, 0.0]  \n",
       "1                         []                 [1.0, 0.0]  \n",
       "2                         []                      [1.0]  \n",
       "3                         []                      [1.0]  \n",
       "4                      [148]            [0.0, 0.0, 0.0]  \n",
       "..                       ...                        ...  \n",
       "685  [13, 32, 151, 159, 166]  [0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "686                      [3]                 [0.0, 1.0]  \n",
       "687                       []                      [1.0]  \n",
       "688                      [3]                      [1.0]  \n",
       "689                       []       [1.0, 0.0, 0.0, 0.0]  \n",
       "\n",
       "[690 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = []\n",
    "labels = []\n",
    "texts = []\n",
    "sent_lens = []\n",
    "sent_nums = []\n",
    "\n",
    "##tokenize words\n",
    "len_tr = len(train_df_preprocessed)\n",
    "# result = train_df_preprocessed.append(extra_train_df, ignore_index=True, sort=False)\n",
    "result = train_df_preprocessed\n",
    "train_data = {\n",
    "    'sentence':  result.sentences.sum(),\n",
    "    'toxicity_sentence': result.toxicity_sentence.sum()\n",
    "        }\n",
    "\n",
    "train_df = pd.DataFrame (train_data, columns = ['sentence','toxicity_sentence'])\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =[]\n",
    "for i in train_df.sentence:\n",
    "    sentences.append(nltk.word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter token that are not alphabetic\n",
    "sentences_filter=[]\n",
    "for i, w in enumerate(sentences):\n",
    "    sentences[i] = [word for word in sentences[i] if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/patrycja/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#filter stop words\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "for i, w in enumerate(sentences):\n",
    "    sentences[i] = [w for w in sentences[i] if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [x for x in sentences if x!=[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_FEATURES,lower=True, split=\" \")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "word_counts = tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word_vectors = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True, limit = 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "vocabulary_size=min(len(word_index)+1,MAX_FEATURES)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "absent_words = 0\n",
    "for word, i in word_index.items():\n",
    "    if i>=MAX_FEATURES:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=0\n",
    "        absent_words+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save toknizer to file so that it could be used again\n",
    "import pickle\n",
    "\n",
    "# saving\n",
    "\n",
    "#with open('tokenizer_nn.pickle', 'wb') as handle:\n",
    "\n",
    "#    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = np.zeros((len(sentences), MAX_WORD_NUM), dtype='int32')\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for k, word in enumerate(sentence):\n",
    "        try:\n",
    "            if k<MAX_WORD_NUM and tokenizer.word_index[word]<MAX_FEATURES:\n",
    "                data_index[i,k] = tokenizer.word_index[word]\n",
    "        except:\n",
    "            #print(word)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  34   63  762    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [1785    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  47  763   18   52  764 1081 1786 1787 1082    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  44 1788 1083  765 1084    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   2  564    6 1085  449 1789  564 1790 1791    4    9   48 1792  179\n",
      "    18 1086    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(data_index[:5])\n",
    "indices = np.arange(data_index.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data_index[indices].copy()\n",
    "##IMPORTANT\n",
    "data = data.astype(np.float32)\n",
    "labels = train_df.toxicity_sentence.iloc[indices]\n",
    "# labels = labels.astype(np.float32)\n",
    "nb_validation_samples = int(VAL_SPLIT * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = np.vstack(labels[:-nb_validation_samples])\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = np.vstack(labels[-nb_validation_samples:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_softmax_train = np.zeros((y_train.shape[0], 2))\n",
    "y_softmax_val = np.zeros((y_val.shape[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, y_softmax_train.shape[0]):\n",
    "    if y_train[i] == 0:\n",
    "        y_softmax_train[i][0] = 1\n",
    "    else :\n",
    "        y_softmax_train[i][1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, y_softmax_val.shape[0]):\n",
    "    if y_val[i] == 0:\n",
    "        y_softmax_val[i][0] = 1\n",
    "    else :\n",
    "        y_softmax_val[i][1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 40, 300)           1349100   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 40, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 600)               1442400   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               180300    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_final (Dense)          (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 3,062,702\n",
      "Trainable params: 3,062,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1 ,EMBEDDING_DIM,weights=[embedding_matrix], input_length=MAX_WORD_NUM, trainable= True, name='embedding'))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(EMBEDDING_DIM,dropout=0.3, recurrent_dropout=0.3), name='bidirectional'))\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu', name='dense'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu', name='dense2'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(2, activation='softmax', name='dense_final'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 40, 300)           1349100   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 40, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 600)               1442400   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               180300    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_final (Dense)          (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 3,062,702\n",
      "Trainable params: 3,062,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc']) ##adam\n",
    "checkpoint = ModelCheckpoint('best_model.h5', verbose=-2, monitor='val_loss',save_best_only=True, mode='auto')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.6912 - acc: 0.5361 - val_loss: 0.6792 - val_acc: 0.6051\n",
      "Epoch 2/25\n",
      "2/2 [==============================] - 5s 2s/step - loss: 0.6867 - acc: 0.5681 - val_loss: 0.6738 - val_acc: 0.6051\n",
      "Epoch 3/25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3530ad5726a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_softmax_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_softmax_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w2w/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w2w/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w2w/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w2w/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w2w/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w2w/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/envs/w2w/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/w2w/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w2w/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_softmax_train, validation_data=(x_val, y_softmax_val), epochs=25, batch_size=1024,shuffle=True, callbacks=[checkpoint])\n",
    "print(history.history.keys())\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from lime import lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "test_data_semeval = SemEvalData(MAX_WORD_NUM)\n",
    "test_data_semeval.load_data(\"data/tsd_trial.csv\")\n",
    "test_df_preprocessed = test_data_semeval.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_df_preprocessed\n",
    "test_data = {\n",
    "    'sentence':  result.sentences.sum(),\n",
    "    'toxicity_sentence': result.toxicity_sentence.sum()\n",
    "        }\n",
    "\n",
    "test_df = pd.DataFrame (test_data, columns = ['sentence','toxicity_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"lstm_drop_jul_train.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 40, 300)           5606100   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 40, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 600)               1442400   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               180300    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_final (Dense)          (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 7,319,702\n",
      "Trainable params: 7,319,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "#y_true = y_test\n",
    "#print(y_pred)\n",
    "#print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting explanation to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxic_words</th>\n",
       "      <th>original_text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>diff</th>\n",
       "      <th>toxicity_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[15, 16, 17, 18, 19, 27, 28, 29, 30, 31]</td>\n",
       "      <td>because hes a moron and a bigot. its not any m...</td>\n",
       "      <td>1</td>\n",
       "      <td>[moron, bigot]</td>\n",
       "      <td>Because he's a moron and a bigot. It's not any...</td>\n",
       "      <td>[because hes a moron and a bigot., its not any...</td>\n",
       "      <td>[10, 36]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[29, 30, 31, 32, 33, 34]</td>\n",
       "      <td>how about we stop protecting idiots and let na...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>How about we stop protecting idiots and let na...</td>\n",
       "      <td>[how about we stop protecting idiots and let n...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[166, 167, 168, 169, 170, 171]</td>\n",
       "      <td>if people  were  smart, they would  boycott th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>If people  were  smart, they would  Boycott th...</td>\n",
       "      <td>[if people  were  smart, they would  boycott t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[87, 88, 89, 90, 91, 92]</td>\n",
       "      <td>trump claimed that russia will never invade th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>Trump Claimed that Russia will never invade th...</td>\n",
       "      <td>[trump claimed that russia will never invade t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>as long as your willing to pay a lot more for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>As long as your willing to pay a lot more for ...</td>\n",
       "      <td>[as long as your willing to pay a lot more for...</td>\n",
       "      <td>[148]</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[8, 9, 10, 11, 12]</td>\n",
       "      <td>only an idiot would use and believe anything t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiot]</td>\n",
       "      <td>Only an idiot would use and believe anything t...</td>\n",
       "      <td>[only an idiot would use and believe anything ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[265, 266, 267, 268, 269, 270, 271, 272, 273, ...</td>\n",
       "      <td>thanks a lot douchebag. youre the reason the p...</td>\n",
       "      <td>1</td>\n",
       "      <td>[o try to turn salem into some kind of new-stu...</td>\n",
       "      <td>Thanks a lot douchebag. You're the reason the ...</td>\n",
       "      <td>[thanks a lot douchebag., youre the reason the...</td>\n",
       "      <td>[27, 467]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[]</td>\n",
       "      <td>kick all the non human criminal illegals out o...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>kick all the non human criminal illegals out o...</td>\n",
       "      <td>[kick all the non human criminal illegals out ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[38, 39, 40, 41, 42, 43]</td>\n",
       "      <td>because driving under ontario laws is stupid e...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>Because driving under Ontario laws is stupid e...</td>\n",
       "      <td>[because driving under ontario laws is stupid ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[277, 278, 279, 280, 281, 282, 283, 284, 285, ...</td>\n",
       "      <td>youre wrong.  the delay between retirement and...</td>\n",
       "      <td>1</td>\n",
       "      <td>[dont make ignorant statements]</td>\n",
       "      <td>You're wrong.  The delay between retirement an...</td>\n",
       "      <td>[youre wrong., the delay between retirement an...</td>\n",
       "      <td>[3, 264, 280]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[26, 27, 28, 29, 30]</td>\n",
       "      <td>billy, are you a complete idiot, being thick h...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiot]</td>\n",
       "      <td>Billy, are you a complete idiot, being thick h...</td>\n",
       "      <td>[billy, are you a complete idiot, being thick ...</td>\n",
       "      <td>[245]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[35, 36, 37, 38, 39, 40, 41, 42, 49, 50, 51, 5...</td>\n",
       "      <td>trump said, in as many words, that mexicans we...</td>\n",
       "      <td>1</td>\n",
       "      <td>[mexicans, rapists, drug dealers]</td>\n",
       "      <td>Trump said, IN AS MANY WORDS, that Mexicans we...</td>\n",
       "      <td>[trump said, in as many words, that mexicans w...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[7, 8, 9, 10, 11, 12, 13, 14, 15, 16]</td>\n",
       "      <td>hes a psychopath.</td>\n",
       "      <td>1</td>\n",
       "      <td>[psychopath]</td>\n",
       "      <td>He's a psychopath.</td>\n",
       "      <td>[hes a psychopath.]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[]</td>\n",
       "      <td>clinton should be the last person to say anyth...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Clinton should be the last person to say anyth...</td>\n",
       "      <td>[clinton should be the last person to say anyt...</td>\n",
       "      <td>[99, 415, 467]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[23, 24, 25, 26]</td>\n",
       "      <td>people insist on being dumb. no other explanat...</td>\n",
       "      <td>1</td>\n",
       "      <td>[dumb]</td>\n",
       "      <td>People insist on being dumb. No other explanat...</td>\n",
       "      <td>[people insist on being dumb., no other explan...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[]</td>\n",
       "      <td>canadians killed by islamic extremists? the im...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Canadians killed by Islamic extremists? The im...</td>\n",
       "      <td>[canadians killed by islamic extremists?, the ...</td>\n",
       "      <td>[462]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[51, 52, 53, 54, 55, 56, 57, 58, 59]</td>\n",
       "      <td>so next year if you apply for yours, then your...</td>\n",
       "      <td>1</td>\n",
       "      <td>[hypocrite]</td>\n",
       "      <td>So next year if you apply for yours, then you'...</td>\n",
       "      <td>[so next year if you apply for yours, then you...</td>\n",
       "      <td>[45]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[103, 104, 105, 106, 107, 108, 109, 110, 111, ...</td>\n",
       "      <td>this new has made the a/p, cnbc, bloomberg,  e...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ridiculous]</td>\n",
       "      <td>This new has made the A/P, CNBC, Bloomberg,  e...</td>\n",
       "      <td>[this new has made the a/p, cnbc, bloomberg,  ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                spans  \\\n",
       "0            [15, 16, 17, 18, 19, 27, 28, 29, 30, 31]   \n",
       "1                            [29, 30, 31, 32, 33, 34]   \n",
       "2                      [166, 167, 168, 169, 170, 171]   \n",
       "3                            [87, 88, 89, 90, 91, 92]   \n",
       "4                                                  []   \n",
       "5                                  [8, 9, 10, 11, 12]   \n",
       "6   [265, 266, 267, 268, 269, 270, 271, 272, 273, ...   \n",
       "7                                                  []   \n",
       "8                            [38, 39, 40, 41, 42, 43]   \n",
       "9   [277, 278, 279, 280, 281, 282, 283, 284, 285, ...   \n",
       "10                               [26, 27, 28, 29, 30]   \n",
       "11  [35, 36, 37, 38, 39, 40, 41, 42, 49, 50, 51, 5...   \n",
       "12              [7, 8, 9, 10, 11, 12, 13, 14, 15, 16]   \n",
       "13                                                 []   \n",
       "14                                   [23, 24, 25, 26]   \n",
       "15                                                 []   \n",
       "16               [51, 52, 53, 54, 55, 56, 57, 58, 59]   \n",
       "17  [103, 104, 105, 106, 107, 108, 109, 110, 111, ...   \n",
       "\n",
       "                                                 text  toxicity  \\\n",
       "0   because hes a moron and a bigot. its not any m...         1   \n",
       "1   how about we stop protecting idiots and let na...         1   \n",
       "2   if people  were  smart, they would  boycott th...         1   \n",
       "3   trump claimed that russia will never invade th...         1   \n",
       "4   as long as your willing to pay a lot more for ...         0   \n",
       "5   only an idiot would use and believe anything t...         1   \n",
       "6   thanks a lot douchebag. youre the reason the p...         1   \n",
       "7   kick all the non human criminal illegals out o...         0   \n",
       "8   because driving under ontario laws is stupid e...         1   \n",
       "9   youre wrong.  the delay between retirement and...         1   \n",
       "10  billy, are you a complete idiot, being thick h...         1   \n",
       "11  trump said, in as many words, that mexicans we...         1   \n",
       "12                                  hes a psychopath.         1   \n",
       "13  clinton should be the last person to say anyth...         0   \n",
       "14  people insist on being dumb. no other explanat...         1   \n",
       "15  canadians killed by islamic extremists? the im...         0   \n",
       "16  so next year if you apply for yours, then your...         1   \n",
       "17  this new has made the a/p, cnbc, bloomberg,  e...         1   \n",
       "\n",
       "                                          toxic_words  \\\n",
       "0                                      [moron, bigot]   \n",
       "1                                            [idiots]   \n",
       "2                                            [idiots]   \n",
       "3                                            [stupid]   \n",
       "4                                                  []   \n",
       "5                                             [idiot]   \n",
       "6   [o try to turn salem into some kind of new-stu...   \n",
       "7                                                  []   \n",
       "8                                            [stupid]   \n",
       "9                     [dont make ignorant statements]   \n",
       "10                                            [idiot]   \n",
       "11                  [mexicans, rapists, drug dealers]   \n",
       "12                                       [psychopath]   \n",
       "13                                                 []   \n",
       "14                                             [dumb]   \n",
       "15                                                 []   \n",
       "16                                        [hypocrite]   \n",
       "17                                       [ridiculous]   \n",
       "\n",
       "                                        original_text  \\\n",
       "0   Because he's a moron and a bigot. It's not any...   \n",
       "1   How about we stop protecting idiots and let na...   \n",
       "2   If people  were  smart, they would  Boycott th...   \n",
       "3   Trump Claimed that Russia will never invade th...   \n",
       "4   As long as your willing to pay a lot more for ...   \n",
       "5   Only an idiot would use and believe anything t...   \n",
       "6   Thanks a lot douchebag. You're the reason the ...   \n",
       "7   kick all the non human criminal illegals out o...   \n",
       "8   Because driving under Ontario laws is stupid e...   \n",
       "9   You're wrong.  The delay between retirement an...   \n",
       "10  Billy, are you a complete idiot, being thick h...   \n",
       "11  Trump said, IN AS MANY WORDS, that Mexicans we...   \n",
       "12                                 He's a psychopath.   \n",
       "13  Clinton should be the last person to say anyth...   \n",
       "14  People insist on being dumb. No other explanat...   \n",
       "15  Canadians killed by Islamic extremists? The im...   \n",
       "16  So next year if you apply for yours, then you'...   \n",
       "17  This new has made the A/P, CNBC, Bloomberg,  e...   \n",
       "\n",
       "                                            sentences            diff  \\\n",
       "0   [because hes a moron and a bigot., its not any...        [10, 36]   \n",
       "1   [how about we stop protecting idiots and let n...              []   \n",
       "2   [if people  were  smart, they would  boycott t...              []   \n",
       "3   [trump claimed that russia will never invade t...              []   \n",
       "4   [as long as your willing to pay a lot more for...           [148]   \n",
       "5   [only an idiot would use and believe anything ...              []   \n",
       "6   [thanks a lot douchebag., youre the reason the...       [27, 467]   \n",
       "7   [kick all the non human criminal illegals out ...              []   \n",
       "8   [because driving under ontario laws is stupid ...              []   \n",
       "9   [youre wrong., the delay between retirement an...   [3, 264, 280]   \n",
       "10  [billy, are you a complete idiot, being thick ...           [245]   \n",
       "11  [trump said, in as many words, that mexicans w...              []   \n",
       "12                                [hes a psychopath.]             [2]   \n",
       "13  [clinton should be the last person to say anyt...  [99, 415, 467]   \n",
       "14  [people insist on being dumb., no other explan...              []   \n",
       "15  [canadians killed by islamic extremists?, the ...           [462]   \n",
       "16  [so next year if you apply for yours, then you...            [45]   \n",
       "17  [this new has made the a/p, cnbc, bloomberg,  ...              []   \n",
       "\n",
       "                                    toxicity_sentence  \n",
       "0                                          [1.0, 0.0]  \n",
       "1                                          [1.0, 0.0]  \n",
       "2                                               [1.0]  \n",
       "3                                               [1.0]  \n",
       "4                                     [0.0, 0.0, 0.0]  \n",
       "5                                               [1.0]  \n",
       "6                 [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]  \n",
       "7                                               [0.0]  \n",
       "8                                               [1.0]  \n",
       "9                 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "10  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "11                                              [1.0]  \n",
       "12                                              [1.0]  \n",
       "13           [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "14                                         [1.0, 0.0]  \n",
       "15                          [0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "16                                              [1.0]  \n",
       "17           [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_preprocessed[:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['hes', 'a'] because hes a moron and a bigot. its not any more complicated than that. []\n",
      "\n",
      "['how', 'we', 'stop', 'protecting'] how about we stop protecting idiots and let nature add some bleach to the gene pool. we can always submit their names for the darwin awards. []\n",
      "\n",
      "[] if people  were  smart, they would  boycott this  inept  airline,  but   they  are  not  smart,  so   rogue  businesses  like  this  one,   still thrive   taking the idiots  for  a ride... []\n",
      "\n",
      "[] trump claimed that russia will never invade the ukraine, when russia already has - how stupid can people be? []\n",
      "\n",
      "['call', 'we', 'get', 'to'] as long as your willing to pay a lot more for products you buy, then fine. but you better not be going to costco and walmart to buy stuff because its cheaper. if so, we get to call you a hypocritical wanker. []\n",
      "\n",
      "['would', 'idiot', 'believe', 'anything'] only an idiot would use and believe anything this this republican propaganda machine publishes: www.realclearpolitics.com []\n",
      "\n",
      "['thanks', 'douchebag', 'buttfuckers', 'all', 'because', 'of', 'fucking', 'outsider', 'with', 'the'] thanks a lot douchebag. youre the reason the portland buttfuckers are getting pushed out and moving down south to the wilsonville area and beyond. because of you all the stupid fucking assholes that made portland a place full of stupid fucking assholes are going to try to turn salem into some kind of new-stupid-fucking-asshole-topia. this will drive up the price of housing and displace  people down here. having lived in san clemente for a time myself i think youre dumb as shit to have moved up here. the fucks wrong with you outsider. i hope bums shit in your yard everyday and never let you walk by without demanding spare  change. []\n",
      "\n",
      "[] kick all the non human criminal illegals out of pa and plenty of room;   fools! []\n",
      "\n",
      "['enough'] because driving under ontario laws is stupid enough in ontario. []\n",
      "\n",
      "[] youre wrong.  the delay between retirement and lobbying is nowhere near 7 years.  this law is specifically aimed at the secdef job.  the purpose of this separation is to ensure firm civilian leadership of the military.  read the article.  you have no idea where ive worked.  dont make ignorant statements. []\n",
      "\n",
      "['insurance', 'without', 'care', 'too', 'pp', 'does', 'grants', 'thru', 'prescription'] billy, are you a complete idiot, being thick headed or just not reading what people have been trying to tell you in response to your soapbox blathering.  a) poor people without insurance need health care too. b) planned parenthood provides womens health care for free to anyone.  pp does these thru donations and grants from the states and fed and charity foundations.  no federal money is allow to be spent on abortions.  the people protesting planned parenthood know this, but have decided that a zygote is a person.  these same people believe that corporations are persons too.  once again, poor women get help with birth control via planned parenthood.  poor people mostly do not have insurance, and less will have insurance as soon as the gop gets done gutting aca.  poor women cannot afford to go to dr for prescription birth control, therefore planned parenthood is necessary. []\n",
      "\n",
      "['words'] trump said, in as many words, that mexicans were rapists and drug dealers. []\n",
      "\n",
      "['hes', 'a'] hes a psychopath. []\n",
      "\n",
      "['it', 'president', 'a', 'she', 'have', 'certainly', 'because', 'fault', 'keep'] clinton should be the last person to say anything about anyone. she is such a lying scum bag. i cant handle just hearing her voice. it would be a lot worse if she was president than we are with the crazy trump. she would have her husband (aka: as the cigar rapist) as her guide. he should have been arrested for doing that to that young girl, the starred eye monica. how can anyone blame her but i certainly wouldnt have let him stick a cigar in me. it is the medias fault because they keep going to get her big lying mouth going. []\n",
      "\n",
      "['people', 'insist', 'no'] people insist on being dumb. no other explanation. []\n",
      "\n",
      "['right'] canadians killed by islamic extremists? the immediate result should be canada cleaning up the islamic world from one end to the other so that there are no more terrorists to be found in it, nor any more people who discriminate against members of non-muslim minorities, nor anyone who prepares for war against israel.  those of the muslim faith who are willing to live in peace, on the other hand, will now be able to do so.  that will send the right message: dont mess with canadians. ever. []\n",
      "\n",
      "['if'] so next year if you apply for yours, then youre a hypocrite, mike. []\n",
      "\n",
      "[] this new has made the a/p, cnbc, bloomberg,  etc.  the pm is an international embarrassment, and looks ridiculous..  harper has to go! now! . . . .     oh, wait... []\n",
      "\n",
      "['those', 'trailer', 'ride', 'morons', 'brakes'] ride those trailer brakes, you morons! []\n",
      "\n",
      "['hypocrisy'] of course you cannot defend your own hypocrisy...i see you are starting of 2017 on the same losing track as always...oh, well, no cure for stupidity. []\n",
      "\n",
      "['completely'] another comment proving yet again how completely brain washed stupid the likes of you have become []\n",
      "\n",
      "[] it didnt take much rope for the village idiot to hang himself. []\n",
      "\n",
      "['signed', 'that', 'fundraising', 'sanders', 'campaign', 'and', 'the', 'dnc'] clinton signed that joint fundraising agreement in august 2015. it was reported on then; why is it an issue now? and there wasnt anything illegal about it, so its not a crime. oh, and the sanders campaign signed the same agreement with the dnc in oct 2015. somehow thats overlooked. though granted, sanders later chose to raise money on his own, not thru the fund. no, clinton hasnt gotten away with so many crimes, because she hasnt committed them. it boggles the mind that after all these revelations, people would want a liar and a crook to be our president, clearly refers to trump; a liar and a crook. []\n",
      "\n",
      "[] move to europe if you want to live with stupid. []\n",
      "\n",
      "['ignorant'] this guy must be from nanakuli.  are all the people there so ignorant and gross?  yuk! []\n",
      "\n",
      "['is', 'ridiculous', 'even', 'americans', 'long'] 465 million is chump change to mylan.  their profits are so ridiculous that it wont even make a dent for them to pay this penalty.  they are still more than eager to deprive hard working people of life saving drugs in order to keep those billions coming in.  how long are americans going to put up with this blatant and unashamed profiteering?  maybe when enough folks die because they get stung by a bee or eat shellfish and cant afford to carry their epipen for emergency life saving epinephrine. []\n",
      "\n",
      "['childish', 'nature'] so in effect the petty childish nature of the dems played out by the most useless president of modern times! , well trump will easily be making any memory of nobama be history very shortly []\n",
      "\n",
      "['people', 'countries'] people are tired of seeing their countries overrun with illegal immigrants, crime , social parasites and welfare refugees and i applaud hungary, eastern europe to protect their citizens and approve of  lepen. duterte is right killing druggies , but should be taken to court and sentenced to death if he had political enemies killed - unless they favored unpunished drug use. . and turkey is a case of islamism and a fascist dictator persecuting his own countrymen. []\n",
      "\n",
      "['the'] trying the education route is best but you face a huge problem in that the average person today is dumber than a doughnut.  stupid, maybe you can do something but dumb, dumb is forever. []\n",
      "\n",
      "[] maybe this a$$hat can hook up with rosalyn clueless, our very special anchorage driver of the day. what a cute couple these two idiots would make. []\n",
      "\n",
      "['puppet', 'will', 'do'] the sock puppet will do whatever pm butts tell him to do. []\n",
      "\n",
      "[] another idiot! []\n",
      "\n",
      "['invoke', 'april', 'a'] invoke a leap year april fools and put this pathetic rapists away for life without parole.  let him rot with stale meals and a saggy cot. []\n",
      "\n",
      "['these', 'all', 'removed', 'be', 'such', 'keep', 'voting', 'for', 'fools', 'those'] these freaking donkeys all need to be removed from office. im so sick and tired of these lifelong politicians who all seem clueless and could never run their own business.   for all you people who marched against our president how about we all march to honolulu hale and demand rail be stopped. oh thats right you only like complaining against the president of the united states and do nothing against all the useless politicians in your own backyard.  such hypocrites. keep voting for those donkeys you fools []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['laziness'] liberalism leads to stupidity and laziness, leading largassitis. []\n",
      "\n",
      "[] yes, remember the idiocy, hydrogen would make petroleum obsolete. []\n",
      "\n",
      "[] beijing millionaires and wanna be beijing millionairesses who squeeze money from the rest of us! and stupid blind white men with blinders! []\n",
      "\n",
      "['article', 'would', 'you', 'so', 'if', 'realize', 'reffer', 'guarantee', 'marijuana', 'alcohol'] are u stupid... did you not read article small amount of marijuana...... massive amounts of meth and herion .... yep blame it on weed.... you obviously  never smoked any. if so you would realize a reffer head would have been to lazy and hungry to steal a boat.. as a matter of fact. i guarantee  alcohol  which is legal same as marijuana  played huge part in this... not defending the meth heads or tweakers out there,  but really your simple minded antics are what is wrong with our community... []\n",
      "\n",
      "[] this clearly is why junior is not qualified to be pm........hes an idiot ! []\n",
      "\n",
      "['include'] section 160 should also be amended to include sexual acts with animals not involving penetration. []\n",
      "\n",
      "[] wythe thinks republican members of the house should settle their differences in caucus meetings â€” which are private.  that statement should scare the h*ll out of  any voter.  another advocate for a state government functioning  behind closed doors is exactly who we do not need in the alaska legislature.  the  system she apparently adores (the secretive standing caucus system)has just about pounded this state into the ground.  also, does she understand caucus membership is all about crushing any minority view and has little to do with party platforms and everything to do with the quid pro quo of trading toady behavior for spending in deals that are done behind closed doors?  stupid is as stupid does.......... []\n",
      "\n",
      "['kept', 'throughout', 'me', 'probably', 'would', 'it', 'cheaper', 'just', 'when', 'just', 'yank', 'and', 'the', 'post', 'it', 'reward'] brilliant reply. kept me in suspense throughout. it would probably be cheaper to just offer a bounty on low-hanging britches tho. when you see them, just yank them down and when the twit turns to strike you just push him backwards and take his pic. post it online and get a $25 reward. if enough of these morons get shamed it might beautify the community more than art which is more subjective. everybody enjoys seeing a fool take a fall. []\n",
      "\n",
      "[] trump and right wing nuts are is such huge hypocrites. theyre really pathetic.   trump was fine with ted nugent joking about killing the president http://www.huffingtonpost.com/entry/trump-ted-nugent-donald-trump_us_592f1ec9e4b09ec37c31577e?k8l []\n",
      "\n",
      "['mean', 'walkable', 'well', 'you', 'for', 'not', 'homeless', 'the', 'ammo', 'parking', 'has', 'people', 'in'] santa barbara feel. you mean walkable, well kept sidewalks. livable, nice even. not the dumping ground for homeless drug addicts and prostitutes that it looks like now. the ammo can has enough parking. i never see that lot full. i do see all the snow from that lot pushed in the way of any one on a bike or walking.  people in anchorage need to get off their butts and walk to a business. their doctors will be happier, and they will be healthier. []\n",
      "\n",
      "['its', 'ridiculous'] its hilarious watching the trumpniks tying themselves into ridiculous knots in order to support this moron no matter what.    i wonder if they have any /idea/ how ridiculous they look to normal people. []\n",
      "\n",
      "[] is t-rump really this naive/stupid? []\n",
      "\n",
      "['we', 'arent'] we arent california or new york .    you obviously dont pay the ever rising taxes, but i do and the personal  incomes in this town  dont support them and im sick of them being raised on a yearly basis to support the freeloaders and things like stupid and unneeded roundabouts so the current mayor can have some sort of legacy after hes booted out of office. []\n",
      "\n",
      "[] try the number 5 million, you idiots. []\n",
      "\n",
      "['good', 'going', 'jack', 'have', 'to', 'it'] hay good going jack ass. its jonny come latelys like your self that cant appreciate a place for what it is.  so you have to share it with everyone else instead of keeping the majesty of the place for those who would find it on there own. and actually deserve it.  good job ****hole.  hope i dont see you up there. []\n",
      "\n",
      "['now', 'that', 'they'] life was so much easier (and fun) just a few months ago for the gop and trump chumps could just sit back and blame everything on president obama and chant lock her up! at their brown shirt rallies.  now that they control the oval office and both houses of congress, they have no alternative to turn on each other for the historic failures of their cheeto-face, sphincter-lipped lying con man-in-chief, and the putin henchmen in his administration. []\n",
      "\n",
      "['what', 'too', 'lie', 'is'] what is too pathetic is the lie you just fell for, no one will loose health care loser, there is to be a national fund to ensure that,,,  you are the far nut job lunatic left fringe of stupidity []\n",
      "\n",
      "['bullshit', 'word', 'it', 'is', 'jeff', 'do'] bull shit. did i spell that right? or is it one word: bullshit. jeff bezos: do you tremble at the prospect of president clinton? []\n",
      "\n",
      "['if', 'so'] so if i were to say that islam is being used to create terrorists, which it is, would that be hatred towards muslims?  the gray area on this stupidity is wide and deep. []\n",
      "\n",
      "['and', 'racist', 'trudeau', 'volcano', 'them'] trudeau and cabinet are the racist scum. throw them all into the volcano, ashes to ashes. []\n",
      "\n",
      "[] trumps comments are stupid, reckless and very dangerous. he is temperamentally and intellectually unfit to be the person responsible for the lives of millions. []\n",
      "\n",
      "[] alaska girls kick ass! []\n",
      "\n",
      "['disturbing', 'how', 'about', 'peace', 'idiots'] how about disturbing the peace??  hope he gets at least ten years without parole.  freaking idiots. []\n",
      "\n",
      "['the', 'place', 'regarding', 'comments', 'other', 'your', 'beloved'] the only thing trump is going to win is a place in the guinnes book of records as the biggest lying and most inept potus in us history. regarding your other comments, the msm and the dnc dont have to manufacture anything as long your beloved twitter-in chief continues making a fool of himself whenever he speaks or tweets. []\n",
      "\n",
      "['crap', 'no', 'stop'] no, stop pulling us into this crap. []\n",
      "\n",
      "['equivalency', 'legitimate', 'is'] false equivalency. obama was born in the us. it is irrelevant what fools believed. and he won both his elections with sizable ec counts and very strong majorities in the national popular vote.  legal is not the same as legitimate. dt is not viewed as earning the trust and confidence of the majority when he cant win a majority of the votes. []\n",
      "\n",
      "['in', 'history'] in history englandmen hated to be listed together even with scots or french, let alone with irish, italian, eastern central european, russian .... now they want to enlist them to be white nationalist with them?  ridiculous and coward bunch. []\n",
      "\n",
      "['these', 'other', 'than', 'they', 'rats', 'than'] having been on hagemeister and seeing first hand the devastation, the only word in your diatribe that is accurate is stupid. another example of this man made ecological disaster is at the south end of umnak island where feral cattle and sheep have disturbed an archaeological site that is considered the oldest in the aleutians. ive been there and helped rebury remains that were exposed from the animal caused erosion. to see literally hundreds of remains, exposed hearths and middens was deeply disturbing.  these animals have no value other than to a bored crabber and should be removed. they are no better than rats. []\n",
      "\n",
      "[] see what happens when you dont have an education. guess he figured the device just helps the guards know when he goes to the bathroom. idiot. []\n",
      "\n",
      "['theocracy'] you mean other than it being a race-based theocracy with a moron at the helm? []\n",
      "\n",
      "['evil'] good!  white christians are evil. []\n",
      "\n",
      "['canada', 'has', 'a', 'if', 'now', 'cheaper', 'will', 'far', 'it', 'i'] what an incredibly weak argument. canada has a long history of where one region or province has disproportionality benefited from another. it structurally exists with equalization payments. thats canada.  if british columbians are now truly terrified of an marine disaster, then hire a risk management consulting firm to model the different scenarios, probabilities and consequences based on facts. its time for some myth busting.  i guarantee you, it will be far cheaper, honest, effective and educational than what you will ever get from any law firm, ngo, or misguided/ignorant government. []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[] he cancelled because bprder security is bad for the cartels that fund his crooked ass! nice try though, wapodn! []\n",
      "\n",
      "['learn'] rebuke convicts idiots, the simple will learn to obey. []\n",
      "\n",
      "['what', 'geez'] geez, what an idiot. []\n",
      "\n",
      "[] oh my here we go again.....another day in the lives of hysterical hypocrite liberals and their russian boogey man conspiracies. democrats obvisiously  so butthurt over trumps win that all they have left is impeachment nonsense and verbal attacks to distract the public from their own gross ignorance and anti america, un american direction of their political party. []\n",
      "\n",
      "['to', 'the', 'pathological', 'guy', 'is', 'a'] if youre dumb enough to believe him on this, then ive got a bridge in brooklyn i will sell you... . the guy is a pathological liar.  dont believe anything he says.  its all lies. []\n",
      "\n",
      "['know', 'really', 'it', 'regards'] i know it....but, damn it.....hes really good at it.  regards, gary []\n",
      "\n",
      "['that'] really?  are you that stupid? []\n",
      "\n",
      "[] we have had alternative energy for 3 generations, are you all so bloody stupid? you think threatening bc is a good thing to do? []\n",
      "\n",
      "['rest', 'criminal', 'you', 'in', 'hell', 'is'] what a piece of crap.  he has a record 2 pages long on court view, yet he was free, and able to kill a dog that was a better person than he ever hoped to be.  rest in hell you creepy criminal, death was too good for you, too bad you took out someone that was superior to your white trash human life while being just the average criminal trash that you were born to be.  alaska is lucky to be one less trashy criminal, but is sad to have lost a noble canine life. []\n",
      "\n",
      "[] cbc doesnt worry about competition thats why the are so mediocre. []\n",
      "\n",
      "['untouchable', 'president'] trump either thinks he is untouchable as president or is dumb as a bag of rocks. []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df_preprocessed[\"predicted_span\"]=[test_lime(sentences) for sentences in test_df_preprocessed[\"original_text\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_preprocessed[\"Pscore\"] = [ 1 if (len(s) == 0 and len(ps) == 0) \n",
    "                             else 0 if len(ps) == 0 \n",
    "                             else len( set(s).intersection(set(ps) ))/ len(set(ps))  for s, ps in zip(test_df_preprocessed[\"spans\"],test_df_preprocessed[\"predicted_span\"]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_preprocessed[\"Rscore\"] = [ 1 if (len(s) == 0 and len(ps) == 0) \n",
    "                             else 0 if len(s) == 0 \n",
    "                             else len( set(s).intersection(set(ps) ))/ len(set(s))  for s, ps in zip(test_df_preprocessed[\"spans\"],test_df_preprocessed[\"predicted_span\"]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_preprocessed[\"Fscore\"] = [ 0 if (p == 0 and r == 0)\n",
    "    else 2 * p *r /(p + r) for p, r in zip(test_df_preprocessed[\"Pscore\"], test_df_preprocessed[\"Rscore\"] )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_score= np.mean(test_df_preprocessed[\"Fscore\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
