{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.SemEvalData import SemEvalData\n",
    "from src.JigsawData import JigsawData\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer,  text_to_word_sequence\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed, Dropout, Flatten, SpatialDropout1D\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from src.Attention import Attention\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from src.preprocessing import get_embeddings_index, get_embeddings_matrix, getSpansByToxicWords\n",
    "from keras import Sequential\n",
    "from test_sentence import preprocess_lstm, test_lime, vectorize, Transform, getPredictedWordsFromSentence\n",
    "import pickle\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##set to .env\n",
    "MAX_FEATURES = 200000 # maximum number of unique words that should be included in the tokenized word index\n",
    "MAX_WORD_NUM = 40     # maximum number of letters in sentence?\n",
    "EMBED_SIZE = 50  ## same value as in dimension of glove\n",
    "VAL_SPLIT = 0.2  \n",
    "REG_PARAM = 1e-13\n",
    "l2_reg = regularizers.l2(REG_PARAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Julia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "train_data_semeval = SemEvalData(MAX_WORD_NUM)\n",
    "train_data_semeval.load_data(\"data/tsd_trial.csv\")\n",
    "train_df_preprocessed = train_data_semeval.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxic_words</th>\n",
       "      <th>original_text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>diff</th>\n",
       "      <th>toxicity_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[15, 16, 17, 18, 19, 27, 28, 29, 30, 31]</td>\n",
       "      <td>because hes a moron and a bigot. its not any m...</td>\n",
       "      <td>1</td>\n",
       "      <td>[moron, bigot]</td>\n",
       "      <td>Because he's a moron and a bigot. It's not any...</td>\n",
       "      <td>[because hes a moron and a bigot., its not any...</td>\n",
       "      <td>[10, 36]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[29, 30, 31, 32, 33, 34]</td>\n",
       "      <td>how about we stop protecting idiots and let na...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>How about we stop protecting idiots and let na...</td>\n",
       "      <td>[how about we stop protecting idiots and let n...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[166, 167, 168, 169, 170, 171]</td>\n",
       "      <td>if people  were  smart, they would  boycott th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>If people  were  smart, they would  Boycott th...</td>\n",
       "      <td>[if people  were  smart, they would  boycott t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[87, 88, 89, 90, 91, 92]</td>\n",
       "      <td>trump claimed that russia will never invade th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>Trump Claimed that Russia will never invade th...</td>\n",
       "      <td>[trump claimed that russia will never invade t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>as long as your willing to pay a lot more for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>As long as your willing to pay a lot more for ...</td>\n",
       "      <td>[as long as your willing to pay a lot more for...</td>\n",
       "      <td>[148]</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>[129, 130, 131, 132, 133, 134]</td>\n",
       "      <td>but ... trumps not bluffing. hes prepared to g...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>But ... Trump's not bluffing. He's prepared to...</td>\n",
       "      <td>[but ... trumps not bluffing., hes prepared to...</td>\n",
       "      <td>[13, 32, 151, 159, 166]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>[126, 127, 128, 129, 130, 131]</td>\n",
       "      <td>cant believe the limited knowledge of this art...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>Can't believe the limited knowledge of this Ar...</td>\n",
       "      <td>[cant believe the limited knowledge of this ar...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>[24, 25, 26, 27, 28, 29]</td>\n",
       "      <td>i think it conservative idiots who cannot reac...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>I think it conservative idiots who cannot reac...</td>\n",
       "      <td>[i think it conservative idiots who cannot rea...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>youre an id*ot...go away.</td>\n",
       "      <td>1</td>\n",
       "      <td>[youre an id*ot]</td>\n",
       "      <td>You're an id*ot...Go away.</td>\n",
       "      <td>[youre an id*ot...go away.]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>[136, 137, 138, 139, 140, 141]</td>\n",
       "      <td>unless there is wording in the employment cont...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>Unless there is wording in the employment cont...</td>\n",
       "      <td>[unless there is wording in the employment con...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 spans  \\\n",
       "0             [15, 16, 17, 18, 19, 27, 28, 29, 30, 31]   \n",
       "1                             [29, 30, 31, 32, 33, 34]   \n",
       "2                       [166, 167, 168, 169, 170, 171]   \n",
       "3                             [87, 88, 89, 90, 91, 92]   \n",
       "4                                                   []   \n",
       "..                                                 ...   \n",
       "685                     [129, 130, 131, 132, 133, 134]   \n",
       "686                     [126, 127, 128, 129, 130, 131]   \n",
       "687                           [24, 25, 26, 27, 28, 29]   \n",
       "688  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "689                     [136, 137, 138, 139, 140, 141]   \n",
       "\n",
       "                                                  text  toxicity  \\\n",
       "0    because hes a moron and a bigot. its not any m...         1   \n",
       "1    how about we stop protecting idiots and let na...         1   \n",
       "2    if people  were  smart, they would  boycott th...         1   \n",
       "3    trump claimed that russia will never invade th...         1   \n",
       "4    as long as your willing to pay a lot more for ...         0   \n",
       "..                                                 ...       ...   \n",
       "685  but ... trumps not bluffing. hes prepared to g...         1   \n",
       "686  cant believe the limited knowledge of this art...         1   \n",
       "687  i think it conservative idiots who cannot reac...         1   \n",
       "688                          youre an id*ot...go away.         1   \n",
       "689  unless there is wording in the employment cont...         1   \n",
       "\n",
       "          toxic_words                                      original_text  \\\n",
       "0      [moron, bigot]  Because he's a moron and a bigot. It's not any...   \n",
       "1            [idiots]  How about we stop protecting idiots and let na...   \n",
       "2            [idiots]  If people  were  smart, they would  Boycott th...   \n",
       "3            [stupid]  Trump Claimed that Russia will never invade th...   \n",
       "4                  []  As long as your willing to pay a lot more for ...   \n",
       "..                ...                                                ...   \n",
       "685          [stupid]  But ... Trump's not bluffing. He's prepared to...   \n",
       "686          [stupid]  Can't believe the limited knowledge of this Ar...   \n",
       "687          [idiots]  I think it conservative idiots who cannot reac...   \n",
       "688  [youre an id*ot]                         You're an id*ot...Go away.   \n",
       "689          [stupid]  Unless there is wording in the employment cont...   \n",
       "\n",
       "                                             sentences  \\\n",
       "0    [because hes a moron and a bigot., its not any...   \n",
       "1    [how about we stop protecting idiots and let n...   \n",
       "2    [if people  were  smart, they would  boycott t...   \n",
       "3    [trump claimed that russia will never invade t...   \n",
       "4    [as long as your willing to pay a lot more for...   \n",
       "..                                                 ...   \n",
       "685  [but ... trumps not bluffing., hes prepared to...   \n",
       "686  [cant believe the limited knowledge of this ar...   \n",
       "687  [i think it conservative idiots who cannot rea...   \n",
       "688                        [youre an id*ot...go away.]   \n",
       "689  [unless there is wording in the employment con...   \n",
       "\n",
       "                        diff          toxicity_sentence  \n",
       "0                   [10, 36]                 [1.0, 0.0]  \n",
       "1                         []                 [1.0, 0.0]  \n",
       "2                         []                      [1.0]  \n",
       "3                         []                      [1.0]  \n",
       "4                      [148]            [0.0, 0.0, 0.0]  \n",
       "..                       ...                        ...  \n",
       "685  [13, 32, 151, 159, 166]  [0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "686                      [3]                 [0.0, 1.0]  \n",
       "687                       []                      [1.0]  \n",
       "688                      [3]                      [1.0]  \n",
       "689                       []       [1.0, 0.0, 0.0, 0.0]  \n",
       "\n",
       "[690 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = []\n",
    "labels = []\n",
    "texts = []\n",
    "sent_lens = []\n",
    "sent_nums = []\n",
    "\n",
    "##tokenize words\n",
    "len_tr = len(train_df_preprocessed)\n",
    "# result = train_df_preprocessed.append(extra_train_df, ignore_index=True, sort=False)\n",
    "result = train_df_preprocessed\n",
    "train_data = {\n",
    "    'sentence':  result.sentences.sum(),\n",
    "    'toxicity_sentence': result.toxicity_sentence.sum()\n",
    "        }\n",
    "\n",
    "train_df = pd.DataFrame (train_data, columns = ['sentence','toxicity_sentence'])\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =[]\n",
    "for i in train_df.sentence:\n",
    "    sentences.append(nltk.word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter token that are not alphabetic\n",
    "sentences_filter=[]\n",
    "for i, w in enumerate(sentences):\n",
    "    sentences[i] = [word for word in sentences[i] if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Julia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#filter stop words\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "for i, w in enumerate(sentences):\n",
    "    sentences[i] = [w for w in sentences[i] if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [x for x in sentences if x!=[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_FEATURES,lower=True, split=\" \")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "word_counts = tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word_vectors = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True, limit = 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "vocabulary_size=min(len(word_index)+1,MAX_FEATURES)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "absent_words = 0\n",
    "for word, i in word_index.items():\n",
    "    if i>=MAX_FEATURES:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=0\n",
    "        absent_words+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save toknizer to file so that it could be used again\n",
    "import pickle\n",
    "\n",
    "# saving\n",
    "\n",
    "# with open('tokenizer_nn.pickle', 'wb') as handle:\n",
    "\n",
    "#    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = np.zeros((len(sentences), MAX_WORD_NUM), dtype='int32')\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for k, word in enumerate(sentence):\n",
    "        try:\n",
    "            if k<MAX_WORD_NUM and tokenizer.word_index[word]<MAX_FEATURES:\n",
    "                data_index[i,k] = tokenizer.word_index[word]\n",
    "        except:\n",
    "            #print(word)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_index[:5])\n",
    "indices = np.arange(data_index.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data_index[indices].copy()\n",
    "##IMPORTANT\n",
    "data = data.astype(np.float32)\n",
    "labels = train_df.toxicity_sentence.iloc[indices]\n",
    "# labels = labels.astype(np.float32)\n",
    "nb_validation_samples = int(VAL_SPLIT * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = np.vstack(labels[:-nb_validation_samples])\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = np.vstack(labels[-nb_validation_samples:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_softmax_train = np.zeros((y_train.shape[0], 2))\n",
    "y_softmax_val = np.zeros((y_val.shape[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, y_softmax_train.shape[0]):\n",
    "    if y_train[i] == 0:\n",
    "        y_softmax_train[i][0] = 1\n",
    "    else :\n",
    "        y_softmax_train[i][1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, y_softmax_val.shape[0]):\n",
    "    if y_val[i] == 0:\n",
    "        y_softmax_val[i][0] = 1\n",
    "    else :\n",
    "        y_softmax_val[i][1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1 ,EMBEDDING_DIM,weights=[embedding_matrix], input_length=MAX_WORD_NUM, trainable= True, name='embedding'))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(EMBEDDING_DIM,dropout=0.3, recurrent_dropout=0.3), name='bidirectional'))\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu', name='dense'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu', name='dense2'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(2, activation='softmax', name='dense_final'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc']) ##adam\n",
    "checkpoint = ModelCheckpoint('best_model.h5', verbose=-2, monitor='val_loss',save_best_only=True, mode='auto')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_softmax_train, validation_data=(x_val, y_softmax_val), epochs=5, batch_size=1024,shuffle=True, callbacks=[checkpoint])\n",
    "print(history.history.keys())\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from lime import lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "test_data_semeval = SemEvalData(MAX_WORD_NUM)\n",
    "test_data_semeval.load_data(\"data/tsd_trial.csv\")\n",
    "test_df_preprocessed = test_data_semeval.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_df_preprocessed\n",
    "test_data = {\n",
    "    'sentence':  result.sentences.sum(),\n",
    "    'toxicity_sentence': result.toxicity_sentence.sum()\n",
    "        }\n",
    "\n",
    "test_df = pd.DataFrame (test_data, columns = ['sentence','toxicity_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"lstm_drop_jul_train.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "#y_true = y_test\n",
    "#print(y_pred)\n",
    "#print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting explanation to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df_preprocessed = test_df_preprocessed[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df_preprocessed.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 9, 10, 11, 27, 28, 29, 30, 31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\julia\\desktop\\wedt\\wedt-project\\env\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_df_preprocessed[\"predicted_span\"]=[test_lime(sentences) for sentences in test_df_preprocessed[\"original_text\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\julia\\desktop\\wedt\\wedt-project\\env\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test_df_preprocessed[\"Pscore\"] = [ 1 if (len(s) == 0 and len(ps) == 0) \n",
    "                             else 0 if len(ps) == 0 \n",
    "                             else len( set(s).intersection(set(ps) ))/ len(set(ps))  for s, ps in zip(test_df_preprocessed[\"spans\"],test_df_preprocessed[\"predicted_span\"]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\julia\\desktop\\wedt\\wedt-project\\env\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test_df_preprocessed[\"Rscore\"] = [ 1 if (len(s) == 0 and len(ps) == 0) \n",
    "                             else 0 if len(s) == 0 \n",
    "                             else len( set(s).intersection(set(ps) ))/ len(set(s))  for s, ps in zip(test_df_preprocessed[\"spans\"],test_df_preprocessed[\"predicted_span\"]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\julia\\desktop\\wedt\\wedt-project\\env\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_df_preprocessed[\"Fscore\"] = [ 0 if (p == 0 and r == 0)\n",
    "    else 2 * p *r /(p + r) for p, r in zip(test_df_preprocessed[\"Pscore\"], test_df_preprocessed[\"Rscore\"] )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxic_words</th>\n",
       "      <th>original_text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>diff</th>\n",
       "      <th>toxicity_sentence</th>\n",
       "      <th>predicted_span</th>\n",
       "      <th>Pscore</th>\n",
       "      <th>Rscore</th>\n",
       "      <th>Fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[15, 16, 17, 18, 19, 27, 28, 29, 30, 31]</td>\n",
       "      <td>because hes a moron and a bigot. its not any m...</td>\n",
       "      <td>1</td>\n",
       "      <td>[moron, bigot]</td>\n",
       "      <td>Because he's a moron and a bigot. It's not any...</td>\n",
       "      <td>[because hes a moron and a bigot., its not any...</td>\n",
       "      <td>[10, 36]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>[8, 9, 10, 11, 27, 28, 29, 30, 31, 21, 22, 23,...</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.740741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[29, 30, 31, 32, 33, 34]</td>\n",
       "      <td>how about we stop protecting idiots and let na...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>How about we stop protecting idiots and let na...</td>\n",
       "      <td>[how about we stop protecting idiots and let n...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>[0, 1, 2, 10, 11, 13, 14, 15, 16, 18, 19, 20, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[166, 167, 168, 169, 170, 171]</td>\n",
       "      <td>if people  were  smart, they would  boycott th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>If people  were  smart, they would  Boycott th...</td>\n",
       "      <td>[if people  were  smart, they would  boycott t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[87, 88, 89, 90, 91, 92]</td>\n",
       "      <td>trump claimed that russia will never invade th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>Trump Claimed that Russia will never invade th...</td>\n",
       "      <td>[trump claimed that russia will never invade t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>as long as your willing to pay a lot more for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>As long as your willing to pay a lot more for ...</td>\n",
       "      <td>[as long as your willing to pay a lot more for...</td>\n",
       "      <td>[148]</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>[177, 178, 179, 180, 167, 168, 170, 171, 172, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[8, 9, 10, 11, 12]</td>\n",
       "      <td>only an idiot would use and believe anything t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiot]</td>\n",
       "      <td>Only an idiot would use and believe anything t...</td>\n",
       "      <td>[only an idiot would use and believe anything ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[14, 15, 16, 17, 18, 8, 9, 10, 11, 12, 28, 29,...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[265, 266, 267, 268, 269, 270, 271, 272, 273, ...</td>\n",
       "      <td>thanks a lot douchebag. youre the reason the p...</td>\n",
       "      <td>1</td>\n",
       "      <td>[o try to turn salem into some kind of new-stu...</td>\n",
       "      <td>Thanks a lot douchebag. You're the reason the ...</td>\n",
       "      <td>[thanks a lot douchebag., youre the reason the...</td>\n",
       "      <td>[27, 467]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 13, 14, 15, 16, 17, 18, 19,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[]</td>\n",
       "      <td>kick all the non human criminal illegals out o...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>kick all the non human criminal illegals out o...</td>\n",
       "      <td>[kick all the non human criminal illegals out ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[38, 39, 40, 41, 42, 43]</td>\n",
       "      <td>because driving under ontario laws is stupid e...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>Because driving under Ontario laws is stupid e...</td>\n",
       "      <td>[because driving under ontario laws is stupid ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[45, 46, 47, 48, 49, 50]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[277, 278, 279, 280, 281, 282, 283, 284, 285, ...</td>\n",
       "      <td>youre wrong.  the delay between retirement and...</td>\n",
       "      <td>1</td>\n",
       "      <td>[dont make ignorant statements]</td>\n",
       "      <td>You're wrong.  The delay between retirement an...</td>\n",
       "      <td>[youre wrong., the delay between retirement an...</td>\n",
       "      <td>[3, 264, 280]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[26, 27, 28, 29, 30]</td>\n",
       "      <td>billy, are you a complete idiot, being thick h...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idiot]</td>\n",
       "      <td>Billy, are you a complete idiot, being thick h...</td>\n",
       "      <td>[billy, are you a complete idiot, being thick ...</td>\n",
       "      <td>[245]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[177, 178, 179, 180, 181, 182, 183, 184, 185, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[35, 36, 37, 38, 39, 40, 41, 42, 49, 50, 51, 5...</td>\n",
       "      <td>trump said, in as many words, that mexicans we...</td>\n",
       "      <td>1</td>\n",
       "      <td>[mexicans, rapists, drug dealers]</td>\n",
       "      <td>Trump said, IN AS MANY WORDS, that Mexicans we...</td>\n",
       "      <td>[trump said, in as many words, that mexicans w...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[23, 24, 25, 26, 27]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[7, 8, 9, 10, 11, 12, 13, 14, 15, 16]</td>\n",
       "      <td>hes a psychopath.</td>\n",
       "      <td>1</td>\n",
       "      <td>[psychopath]</td>\n",
       "      <td>He's a psychopath.</td>\n",
       "      <td>[hes a psychopath.]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[]</td>\n",
       "      <td>clinton should be the last person to say anyth...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Clinton should be the last person to say anyth...</td>\n",
       "      <td>[clinton should be the last person to say anyt...</td>\n",
       "      <td>[99, 415, 467]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[133, 134, 168, 169, 170, 171, 172, 173, 174, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[23, 24, 25, 26]</td>\n",
       "      <td>people insist on being dumb. no other explanat...</td>\n",
       "      <td>1</td>\n",
       "      <td>[dumb]</td>\n",
       "      <td>People insist on being dumb. No other explanat...</td>\n",
       "      <td>[people insist on being dumb., no other explan...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 29, 30]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[]</td>\n",
       "      <td>canadians killed by islamic extremists? the im...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Canadians killed by Islamic extremists? The im...</td>\n",
       "      <td>[canadians killed by islamic extremists?, the ...</td>\n",
       "      <td>[462]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[444, 445, 446, 447, 448]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[51, 52, 53, 54, 55, 56, 57, 58, 59]</td>\n",
       "      <td>so next year if you apply for yours, then your...</td>\n",
       "      <td>1</td>\n",
       "      <td>[hypocrite]</td>\n",
       "      <td>So next year if you apply for yours, then you'...</td>\n",
       "      <td>[so next year if you apply for yours, then you...</td>\n",
       "      <td>[45]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[13, 14]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[103, 104, 105, 106, 107, 108, 109, 110, 111, ...</td>\n",
       "      <td>this new has made the a/p, cnbc, bloomberg,  e...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ridiculous]</td>\n",
       "      <td>This new has made the A/P, CNBC, Bloomberg,  e...</td>\n",
       "      <td>[this new has made the a/p, cnbc, bloomberg,  ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                spans  \\\n",
       "0            [15, 16, 17, 18, 19, 27, 28, 29, 30, 31]   \n",
       "1                            [29, 30, 31, 32, 33, 34]   \n",
       "2                      [166, 167, 168, 169, 170, 171]   \n",
       "3                            [87, 88, 89, 90, 91, 92]   \n",
       "4                                                  []   \n",
       "5                                  [8, 9, 10, 11, 12]   \n",
       "6   [265, 266, 267, 268, 269, 270, 271, 272, 273, ...   \n",
       "7                                                  []   \n",
       "8                            [38, 39, 40, 41, 42, 43]   \n",
       "9   [277, 278, 279, 280, 281, 282, 283, 284, 285, ...   \n",
       "10                               [26, 27, 28, 29, 30]   \n",
       "11  [35, 36, 37, 38, 39, 40, 41, 42, 49, 50, 51, 5...   \n",
       "12              [7, 8, 9, 10, 11, 12, 13, 14, 15, 16]   \n",
       "13                                                 []   \n",
       "14                                   [23, 24, 25, 26]   \n",
       "15                                                 []   \n",
       "16               [51, 52, 53, 54, 55, 56, 57, 58, 59]   \n",
       "17  [103, 104, 105, 106, 107, 108, 109, 110, 111, ...   \n",
       "\n",
       "                                                 text  toxicity  \\\n",
       "0   because hes a moron and a bigot. its not any m...         1   \n",
       "1   how about we stop protecting idiots and let na...         1   \n",
       "2   if people  were  smart, they would  boycott th...         1   \n",
       "3   trump claimed that russia will never invade th...         1   \n",
       "4   as long as your willing to pay a lot more for ...         0   \n",
       "5   only an idiot would use and believe anything t...         1   \n",
       "6   thanks a lot douchebag. youre the reason the p...         1   \n",
       "7   kick all the non human criminal illegals out o...         0   \n",
       "8   because driving under ontario laws is stupid e...         1   \n",
       "9   youre wrong.  the delay between retirement and...         1   \n",
       "10  billy, are you a complete idiot, being thick h...         1   \n",
       "11  trump said, in as many words, that mexicans we...         1   \n",
       "12                                  hes a psychopath.         1   \n",
       "13  clinton should be the last person to say anyth...         0   \n",
       "14  people insist on being dumb. no other explanat...         1   \n",
       "15  canadians killed by islamic extremists? the im...         0   \n",
       "16  so next year if you apply for yours, then your...         1   \n",
       "17  this new has made the a/p, cnbc, bloomberg,  e...         1   \n",
       "\n",
       "                                          toxic_words  \\\n",
       "0                                      [moron, bigot]   \n",
       "1                                            [idiots]   \n",
       "2                                            [idiots]   \n",
       "3                                            [stupid]   \n",
       "4                                                  []   \n",
       "5                                             [idiot]   \n",
       "6   [o try to turn salem into some kind of new-stu...   \n",
       "7                                                  []   \n",
       "8                                            [stupid]   \n",
       "9                     [dont make ignorant statements]   \n",
       "10                                            [idiot]   \n",
       "11                  [mexicans, rapists, drug dealers]   \n",
       "12                                       [psychopath]   \n",
       "13                                                 []   \n",
       "14                                             [dumb]   \n",
       "15                                                 []   \n",
       "16                                        [hypocrite]   \n",
       "17                                       [ridiculous]   \n",
       "\n",
       "                                        original_text  \\\n",
       "0   Because he's a moron and a bigot. It's not any...   \n",
       "1   How about we stop protecting idiots and let na...   \n",
       "2   If people  were  smart, they would  Boycott th...   \n",
       "3   Trump Claimed that Russia will never invade th...   \n",
       "4   As long as your willing to pay a lot more for ...   \n",
       "5   Only an idiot would use and believe anything t...   \n",
       "6   Thanks a lot douchebag. You're the reason the ...   \n",
       "7   kick all the non human criminal illegals out o...   \n",
       "8   Because driving under Ontario laws is stupid e...   \n",
       "9   You're wrong.  The delay between retirement an...   \n",
       "10  Billy, are you a complete idiot, being thick h...   \n",
       "11  Trump said, IN AS MANY WORDS, that Mexicans we...   \n",
       "12                                 He's a psychopath.   \n",
       "13  Clinton should be the last person to say anyth...   \n",
       "14  People insist on being dumb. No other explanat...   \n",
       "15  Canadians killed by Islamic extremists? The im...   \n",
       "16  So next year if you apply for yours, then you'...   \n",
       "17  This new has made the A/P, CNBC, Bloomberg,  e...   \n",
       "\n",
       "                                            sentences            diff  \\\n",
       "0   [because hes a moron and a bigot., its not any...        [10, 36]   \n",
       "1   [how about we stop protecting idiots and let n...              []   \n",
       "2   [if people  were  smart, they would  boycott t...              []   \n",
       "3   [trump claimed that russia will never invade t...              []   \n",
       "4   [as long as your willing to pay a lot more for...           [148]   \n",
       "5   [only an idiot would use and believe anything ...              []   \n",
       "6   [thanks a lot douchebag., youre the reason the...       [27, 467]   \n",
       "7   [kick all the non human criminal illegals out ...              []   \n",
       "8   [because driving under ontario laws is stupid ...              []   \n",
       "9   [youre wrong., the delay between retirement an...   [3, 264, 280]   \n",
       "10  [billy, are you a complete idiot, being thick ...           [245]   \n",
       "11  [trump said, in as many words, that mexicans w...              []   \n",
       "12                                [hes a psychopath.]             [2]   \n",
       "13  [clinton should be the last person to say anyt...  [99, 415, 467]   \n",
       "14  [people insist on being dumb., no other explan...              []   \n",
       "15  [canadians killed by islamic extremists?, the ...           [462]   \n",
       "16  [so next year if you apply for yours, then you...            [45]   \n",
       "17  [this new has made the a/p, cnbc, bloomberg,  ...              []   \n",
       "\n",
       "                                    toxicity_sentence  \\\n",
       "0                                          [1.0, 0.0]   \n",
       "1                                          [1.0, 0.0]   \n",
       "2                                               [1.0]   \n",
       "3                                               [1.0]   \n",
       "4                                     [0.0, 0.0, 0.0]   \n",
       "5                                               [1.0]   \n",
       "6                 [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]   \n",
       "7                                               [0.0]   \n",
       "8                                               [1.0]   \n",
       "9                 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "10  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "11                                              [1.0]   \n",
       "12                                              [1.0]   \n",
       "13           [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "14                                         [1.0, 0.0]   \n",
       "15                          [0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "16                                              [1.0]   \n",
       "17           [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "\n",
       "                                       predicted_span    Pscore  Rscore  \\\n",
       "0   [8, 9, 10, 11, 27, 28, 29, 30, 31, 21, 22, 23,...  0.588235     1.0   \n",
       "1   [0, 1, 2, 10, 11, 13, 14, 15, 16, 18, 19, 20, ...  0.000000     0.0   \n",
       "2                                                  []  0.000000     0.0   \n",
       "3                                                  []  0.000000     0.0   \n",
       "4   [177, 178, 179, 180, 167, 168, 170, 171, 172, ...  0.000000     0.0   \n",
       "5   [14, 15, 16, 17, 18, 8, 9, 10, 11, 12, 28, 29,...  0.200000     1.0   \n",
       "6   [0, 1, 2, 3, 4, 5, 13, 14, 15, 16, 17, 18, 19,...  0.000000     0.0   \n",
       "7                                                  []  1.000000     1.0   \n",
       "8                            [45, 46, 47, 48, 49, 50]  0.000000     0.0   \n",
       "9                                                  []  0.000000     0.0   \n",
       "10  [177, 178, 179, 180, 181, 182, 183, 184, 185, ...  0.000000     0.0   \n",
       "11                               [23, 24, 25, 26, 27]  0.000000     0.0   \n",
       "12                                       [0, 1, 2, 3]  0.000000     0.0   \n",
       "13  [133, 134, 168, 169, 170, 171, 172, 173, 174, ...  0.000000     0.0   \n",
       "14    [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 29, 30]  0.000000     0.0   \n",
       "15                          [444, 445, 446, 447, 448]  0.000000     0.0   \n",
       "16                                           [13, 14]  0.000000     0.0   \n",
       "17                                                 []  0.000000     0.0   \n",
       "\n",
       "      Fscore  \n",
       "0   0.740741  \n",
       "1   0.000000  \n",
       "2   0.000000  \n",
       "3   0.000000  \n",
       "4   0.000000  \n",
       "5   0.333333  \n",
       "6   0.000000  \n",
       "7   1.000000  \n",
       "8   0.000000  \n",
       "9   0.000000  \n",
       "10  0.000000  \n",
       "11  0.000000  \n",
       "12  0.000000  \n",
       "13  0.000000  \n",
       "14  0.000000  \n",
       "15  0.000000  \n",
       "16  0.000000  \n",
       "17  0.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_score= np.mean(test_df_preprocessed[\"Fscore\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11522633744855969"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
