{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from src.SemEvalData import SemEvalData\n",
    "from src.JigsawData import JigsawData\n",
    "from src.preprocessing import preprocess_bayes, getSpansByToxicWords, getToxicWordsBayes\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Julia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmer = nltk.stem.SnowballStemmer('english')\n",
    "lemmer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "train_data_semeval = SemEvalData()\n",
    "train_data_semeval.load_data(\"data/tsd_trial.csv\")\n",
    "train_df_preprocessed = train_data_semeval.preprocess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moron', 'bigot']\n",
      "['idiots']\n",
      "['idiots']\n",
      "['stupid']\n",
      "[]\n",
      "['idiot']\n",
      "['o try to turn salem into some kind of new-stupid-fucking-asshole-topia. this will drive up the price of housing and displace  people down here. having lived in san clemente for a time myself i think youre dumb as shit to have moved up here. the fuck']\n",
      "[]\n",
      "['stupid']\n",
      "['dont make ignorant statements']\n",
      "['idiot']\n",
      "['mexicans', 'rapists', 'drug dealers']\n",
      "['psychopath']\n",
      "[]\n",
      "['dumb']\n",
      "[]\n",
      "['hypocrite']\n",
      "['ridiculous']\n",
      "['morons']\n",
      "['hypocrisy', 'stupidity']\n",
      "['stupid']\n",
      "['idiot']\n",
      "['r cho']\n",
      "['stupid']\n",
      "['nanakuli', 'ignorant']\n",
      "['ridiculous']\n",
      "['useless']\n",
      "['illegal immigrants, crime , social parasites and welfare refugees', 'duterte is right killing druggies', 'islamism and a fascist dictator']\n",
      "[]\n",
      "['idiots']\n",
      "['the sock puppet']\n",
      "['idiot']\n",
      "['put this pathetic rapists', 'let him rot with stale meals and a saggy cot']\n",
      "[]\n",
      "['liberalism leads to stupidity', 'laziness', 'largassitis']\n",
      "['idiocy']\n",
      "['stupid blind', 'white']\n",
      "['stupid']\n",
      "['idiot']\n",
      "['sexual', 'acts with animals']\n",
      "['stupid is as stupid does']\n",
      "[]\n",
      "['nuts', 'pathetic']\n",
      "['drug addicts', 'prostitutes']\n",
      "['trumpniks', 'ridiculous', 'moron', 'ridiculous']\n",
      "['stupid']\n",
      "['stupid']\n",
      "['idiots']\n",
      "['jack ass', '****hole']\n",
      "['cheeto-face, sphincter-lipped lying']\n",
      "['stupidity']\n",
      "['shit', 'bullshit']\n",
      "['stupidity']\n",
      "[]\n",
      "['stupid', 'unfit']\n",
      "['kick ass']\n",
      "['idiots']\n",
      "['potus']\n",
      "['stop pulling us into this crap']\n",
      "['fools']\n",
      "['ridiculous', 'coward']\n",
      "['stupid']\n",
      "['idiot']\n",
      "['moron']\n",
      "['evil']\n",
      "['misguided/ignorant']\n",
      "['crooked ass']\n",
      "['idiots']\n",
      "['idiot']\n",
      "[]\n",
      "['dumb', 'liar']\n",
      "['damn it']\n",
      "['stupid']\n",
      "['bloody stupid']\n",
      "['crap', 'pages']\n",
      "['mediocr']\n",
      "['dumb']\n",
      "['dumb', 'fakin']\n",
      "['silly', 'clowns', 'killer clowns']\n",
      "['stupid', 'stupid']\n",
      "['long', 'bitch']\n",
      "[]\n",
      "['stupid']\n",
      "['ld beg']\n",
      "['ridiculous']\n",
      "['you suck']\n",
      "['weak blood', 'loser']\n",
      "[]\n",
      "['racist', 'silly', 'hypocrites']\n",
      "['piece of dung']\n",
      "['idiots']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for v in train_df_preprocessed['toxic_words']:\n",
    "    if( i > 90):\n",
    "        break\n",
    "    print(v)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "    'sentence':  train_df_preprocessed.sentences.sum(),\n",
    "    'toxicity_sentence': train_df_preprocessed.toxicity_sentence.sum()\n",
    "        }\n",
    "\n",
    "train_df = pd.DataFrame (train_data, columns = ['sentence','toxicity_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  toxicity_sentence\n",
      "0                    because hes a moron and a bigot                1.0\n",
      "1             its not any more complicated than that                0.0\n",
      "2  how about we stop protecting idiots and let na...                1.0\n",
      "3  we can always submit their names for the darwi...                0.0\n",
      "4  if people  were  smart they would  boycott thi...                1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df['sentence'] = train_df.apply(lambda row: preprocess_bayes(row.sentence), axis=1)\n",
    "print(train_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    # Stem words\n",
    "    #data[i] = ' '.join([stemmer.stem(word) for word in data[i].split()])\n",
    "    data = ' '.join([lemmer.lemmatize(word) for word in text.split()])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a grid search to find the best hyperparameters\n",
    "def grid_search(train):\n",
    "    # Create a pipeline\n",
    "    clf_pipeline = sklearn.pipeline.Pipeline([\n",
    "        ('v', CountVectorizer(strip_accents='ascii', stop_words='english')),\n",
    "        ('t', TfidfTransformer()), \n",
    "        ('c', sklearn.naive_bayes.MultinomialNB(fit_prior=True, class_prior=None))\n",
    "        ])\n",
    "    # Set parameters (name in pipeline + name of parameter)\n",
    "    parameters = { \n",
    "        'v__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)], \n",
    "        'v__lowercase': (True, False), \n",
    "        't__use_idf': (True, False), \n",
    "        'c__alpha': (0.3, 0.6, 1.0) }\n",
    "    # Create a grid search classifier\n",
    "    gs_classifier = sklearn.model_selection.GridSearchCV(clf_pipeline, parameters, cv=5, iid=False, n_jobs=2, scoring='accuracy', verbose=1)\n",
    "    \n",
    "    # Start a search (Warning: takes a long time if the whole dataset is used)\n",
    "    # Slice: (train.data[:4000], train.target[:4000])\n",
    "    gs_classifier = gs_classifier.fit(train.data, train.target)\n",
    "    # Print results\n",
    "    print('---- Results ----')\n",
    "    print('Best score: ' + str(gs_classifier.best_score_))\n",
    "    for name in sorted(parameters.keys()):\n",
    "        print('{0}: {1}'.format(name, gs_classifier.best_params_[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sentence'] = train_df.apply(lambda row: stemming(row.sentence), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  toxicity_sentence\n",
      "0                     because he a moron and a bigot                1.0\n",
      "1              it not any more complicated than that                0.0\n",
      "2  how about we stop protecting idiot and let nat...                1.0\n",
      "3  we can always submit their name for the darwin...                0.0\n",
      "4  if people were smart they would boycott this i...                1.0\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate a model\n",
    "def train_and_evaluate(train):\n",
    "    \n",
    "    # Convert to bag of words\n",
    "    count_vect = CountVectorizer(strip_accents='ascii', stop_words='english', lowercase=True, ngram_range=(1,1))\n",
    "    X = count_vect.fit_transform(train['sentence'])\n",
    "    # Convert from occurrences to frequencies\n",
    "    # Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "    # To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "    transformer = TfidfTransformer()\n",
    "    X = transformer.fit_transform(X)\n",
    "    # Create a model\n",
    "    # get the first vector out (for the first document) \n",
    "    first_vector_tfidfvectorizer=X[0] \n",
    " \n",
    "    # place tf-idf values in a pandas data frame \n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=count_vect.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df.sort_values(by=[\"tfidf\"],ascending=True)\n",
    "    print(df.head)\n",
    "    model = sklearn.naive_bayes.MultinomialNB(alpha=0.3, fit_prior=True, class_prior=None)\n",
    "    # Train the model\n",
    "    model.fit(X, train['toxicity_sentence'])\n",
    "    # Save models\n",
    "    joblib.dump(count_vect, 'vectorizer.jbl')\n",
    "    joblib.dump(transformer, 'transformer.jbl')\n",
    "    joblib.dump(model, 'model.jbl')\n",
    "    # Evaluate on training data\n",
    "    print('-- Training data --')\n",
    "    predictions = model.predict(X)\n",
    "    print('here',model.coef_, len(count_vect.get_feature_names()), predictions)\n",
    "#     neg_class_prob_sorted = model.feature_log_prob_[0, :].argsort()\n",
    "#     pos_class_prob_sorted = model.feature_log_prob_[1, :].argsort()\n",
    "\n",
    "#     print(np.take(count_vect.get_feature_names(), neg_class_prob_sorted[:10]))\n",
    "#     print(np.take(count_vect.get_feature_names(), pos_class_prob_sorted[:10]))\n",
    "    print(predictions,X, train['sentence'])\n",
    "    accuracy = sklearn.metrics.accuracy_score(train['toxicity_sentence'], predictions)\n",
    "    print('Accuracy: {0:.2f}'.format(accuracy * 100.0))\n",
    "    print('Classification Report:')\n",
    "    print(sklearn.metrics.classification_report(train['toxicity_sentence'], predictions))\n",
    "    print('')\n",
    "#     Evaluate with 10-fold CV\n",
    "    print('-- 10-fold CV --')\n",
    "    predictions = sklearn.model_selection.cross_val_predict(model, X, train['toxicity_sentence'], cv=10)\n",
    "    accuracy = sklearn.metrics.accuracy_score(train['toxicity_sentence'], predictions)\n",
    "    print('Accuracy: {0:.2f}'.format(accuracy * 100.0))\n",
    "    print('Classification Report:')\n",
    "    print(sklearn.metrics.classification_report(train['toxicity_sentence'], predictions))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of           tfidf\n",
      "aa          0.0\n",
      "ability     0.0\n",
      "abject      0.0\n",
      "able        0.0\n",
      "abortion    0.0\n",
      "...         ...\n",
      "zion        0.0\n",
      "zip         0.0\n",
      "zombie      0.0\n",
      "zone        0.0\n",
      "zygote      0.0\n",
      "\n",
      "[4125 rows x 1 columns]>\n",
      "-- Training data --\n",
      "here [[-9.22107255 -8.51797274 -9.22107255 ... -9.22107255 -9.22107255\n",
      "  -9.22107255]] 4125 [1. 0. 1. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]   (0, 2419)\t0.5829675030880128\n",
      "  (0, 354)\t0.8124954709678864\n",
      "  (1, 725)\t1.0\n",
      "  (2, 3527)\t0.2710466012552448\n",
      "  (2, 2902)\t0.3578505509617729\n",
      "  (2, 2797)\t0.37212675849741617\n",
      "  (2, 2479)\t0.3467770641029903\n",
      "  (2, 2167)\t0.2689345826546366\n",
      "  (2, 1848)\t0.1986127568357092\n",
      "  (2, 1551)\t0.3922479429093661\n",
      "  (2, 379)\t0.3922479429093661\n",
      "  (2, 46)\t0.3467770641029903\n",
      "  (3, 3564)\t0.6233413233842592\n",
      "  (3, 900)\t0.5686786636936917\n",
      "  (3, 262)\t0.5367030575850278\n",
      "  (4, 3704)\t0.3042103656034832\n",
      "  (4, 3624)\t0.23525102568681291\n",
      "  (4, 3383)\t0.5550665027433567\n",
      "  (4, 3137)\t0.3042103656034832\n",
      "  (4, 3121)\t0.28860525415042265\n",
      "  (4, 2713)\t0.14826955977356115\n",
      "  (4, 2191)\t0.15602996429656896\n",
      "  (4, 1923)\t0.2619281399186178\n",
      "  (4, 1848)\t0.15403537599805242\n",
      "  (4, 494)\t0.2508561371398734\n",
      "  :\t:\n",
      "  (1988, 3199)\t0.302106307662759\n",
      "  (1988, 1441)\t0.302106307662759\n",
      "  (1988, 1232)\t0.3793479162809023\n",
      "  (1988, 778)\t0.3793479162809023\n",
      "  (1988, 200)\t0.3460817661411882\n",
      "  (1988, 164)\t0.3460817661411882\n",
      "  (1989, 4081)\t0.45063995094131154\n",
      "  (1989, 1137)\t0.5347033728995519\n",
      "  (1989, 685)\t0.5207525240705856\n",
      "  (1989, 525)\t0.4897272162128313\n",
      "  (1990, 3007)\t0.3848422225336741\n",
      "  (1990, 1701)\t0.4759250659128041\n",
      "  (1990, 904)\t0.37673336375585287\n",
      "  (1990, 422)\t0.5655623714731229\n",
      "  (1990, 274)\t0.4044786422026913\n",
      "  (1991, 4009)\t0.2911522177048539\n",
      "  (1991, 3316)\t0.30276754000419365\n",
      "  (1991, 3011)\t0.4703596369804363\n",
      "  (1991, 2964)\t0.24679514078955794\n",
      "  (1991, 2883)\t0.31913841731217174\n",
      "  (1991, 2615)\t0.31913841731217174\n",
      "  (1991, 2201)\t0.30276754000419365\n",
      "  (1991, 1550)\t0.2821426737821326\n",
      "  (1991, 1441)\t0.25415647417481474\n",
      "  (1991, 1317)\t0.31913841731217174 0                          because he a moron and a bigot\n",
      "1                   it not any more complicated than that\n",
      "2       how about we stop protecting idiot and let nat...\n",
      "3       we can always submit their name for the darwin...\n",
      "4       if people were smart they would boycott this i...\n",
      "                              ...                        \n",
      "1987                                 youre an idotgo away\n",
      "1988    unless there is wording in the employment cont...\n",
      "1989                   or for driving the wrong color car\n",
      "1990        or because our bos is having a really bad day\n",
      "1991    in oregon you can be fired for any reason what...\n",
      "Name: sentence, Length: 1992, dtype: object\n",
      "Accuracy: 96.34\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.98      0.97      1209\n",
      "         1.0       0.96      0.95      0.95       783\n",
      "\n",
      "    accuracy                           0.96      1992\n",
      "   macro avg       0.96      0.96      0.96      1992\n",
      "weighted avg       0.96      0.96      0.96      1992\n",
      "\n",
      "\n",
      "-- 10-fold CV --\n",
      "Accuracy: 69.93\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.79      0.76      1209\n",
      "         1.0       0.63      0.56      0.59       783\n",
      "\n",
      "    accuracy                           0.70      1992\n",
      "   macro avg       0.68      0.67      0.68      1992\n",
      "weighted avg       0.69      0.70      0.70      1992\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_and_evaluate(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load from file\n",
    "count_vect = joblib.load('vectorizer.jbl')\n",
    "transformer = joblib.load('transformer.jbl')\n",
    "model = joblib.load('model.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas(Index=0, sentence='because he a moron and a bigot', toxicity_sentence=1.0)\n",
      "['bigot' 'moron'] because he a moron and a bigot\n",
      "bigot\n",
      "5\n",
      "5\n",
      "moron\n",
      "5\n",
      "5\n",
      "[25, 26, 27, 28, 29, 13, 14, 15, 16, 17]\n",
      "Pandas(Index=1, sentence='it not any more complicated than that', toxicity_sentence=0.0)\n",
      "Pandas(Index=2, sentence='how about we stop protecting idiot and let nature add some bleach to the gene pool', toxicity_sentence=1.0)\n",
      "['add' 'bleach' 'gene' 'idiot' 'let' 'nature' 'pool' 'protecting' 'stop'] how about we stop protecting idiot and let nature add some bleach to the gene pool\n",
      "add\n",
      "3\n",
      "3\n",
      "bleach\n",
      "6\n",
      "6\n",
      "gene\n",
      "4\n",
      "4\n",
      "idiot\n",
      "5\n",
      "5\n",
      "let\n",
      "3\n",
      "3\n",
      "nature\n",
      "6\n",
      "6\n",
      "pool\n",
      "4\n",
      "4\n",
      "protecting\n",
      "10\n",
      "10\n",
      "stop\n",
      "4\n",
      "4\n",
      "[50, 51, 52, 59, 60, 61, 62, 63, 64, 73, 74, 75, 76, 29, 30, 31, 32, 33, 39, 40, 41, 43, 44, 45, 46, 47, 48, 78, 79, 80, 81, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 13, 14, 15, 16]\n",
      "Pandas(Index=3, sentence='we can always submit their name for the darwin award', toxicity_sentence=0.0)\n",
      "Pandas(Index=4, sentence='if people were smart they would boycott this inept airline but they are not smart so rogue business like this one still thrive taking the idiot for a ride', toxicity_sentence=1.0)\n",
      "['airline' 'boycott' 'business' 'idiot' 'inept' 'like' 'people' 'ride'\n",
      " 'rogue' 'smart' 'taking' 'thrive'] if people were smart they would boycott this inept airline but they are not smart so rogue business like this one still thrive taking the idiot for a ride\n",
      "airline\n",
      "7\n",
      "7\n",
      "boycott\n",
      "7\n",
      "7\n",
      "business\n",
      "8\n",
      "8\n",
      "idiot\n",
      "5\n",
      "5\n",
      "inept\n",
      "5\n",
      "5\n",
      "like\n",
      "4\n",
      "4\n",
      "people\n",
      "6\n",
      "6\n",
      "ride\n",
      "4\n",
      "4\n",
      "rogue\n",
      "5\n",
      "5\n",
      "smart\n",
      "5\n",
      "5\n",
      "taking\n",
      "6\n",
      "6\n",
      "thrive\n",
      "6\n",
      "6\n",
      "[51, 52, 53, 54, 55, 56, 57, 32, 33, 34, 35, 36, 37, 38, 91, 92, 93, 94, 95, 96, 97, 98, 138, 139, 140, 141, 142, 45, 46, 47, 48, 49, 100, 101, 102, 103, 3, 4, 5, 6, 7, 8, 150, 151, 152, 153, 85, 86, 87, 88, 89, 15, 16, 17, 18, 19, 127, 128, 129, 130, 131, 132, 120, 121, 122, 123, 124, 125]\n",
      "Pandas(Index=5, sentence='trump claimed that russia will never invade the ukraine when russia already ha how stupid can people be', toxicity_sentence=1.0)\n",
      "['claimed' 'ha' 'invade' 'people' 'russia' 'stupid' 'trump' 'ukraine'] trump claimed that russia will never invade the ukraine when russia already ha how stupid can people be\n",
      "claimed\n",
      "7\n",
      "7\n",
      "ha\n",
      "2\n",
      "2\n",
      "invade\n",
      "6\n",
      "6\n",
      "people\n",
      "6\n",
      "6\n",
      "russia\n",
      "6\n",
      "6\n",
      "stupid\n",
      "6\n",
      "6\n",
      "trump\n",
      "5\n",
      "5\n",
      "ukraine\n",
      "7\n",
      "7\n",
      "[6, 7, 8, 9, 10, 11, 12, 15, 16, 37, 38, 39, 40, 41, 42, 94, 95, 96, 97, 98, 99, 19, 20, 21, 22, 23, 24, 83, 84, 85, 86, 87, 88, 0, 1, 2, 3, 4, 48, 49, 50, 51, 52, 53, 54]\n",
      "Pandas(Index=6, sentence='a long a your willing to pay a lot more for product you buy then fine', toxicity_sentence=0.0)\n",
      "Pandas(Index=7, sentence='but you better not be going to costco and walmart to buy stuff because it cheaper', toxicity_sentence=0.0)\n",
      "Pandas(Index=8, sentence='if so we get to call you a hypocritical wanker', toxicity_sentence=0.0)\n",
      "['hypocritical' 'wanker'] if so we get to call you a hypocritical wanker\n",
      "hypocritical\n",
      "12\n",
      "12\n",
      "wanker\n",
      "6\n",
      "6\n",
      "[27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45]\n",
      "Pandas(Index=9, sentence='only an idiot would use and believe anything this this republican propaganda machine publishes wwwrealclearpoliticscom', toxicity_sentence=1.0)\n",
      "['believe' 'idiot' 'machine' 'propaganda' 'publishes' 'republican' 'use'\n",
      " 'wwwrealclearpoliticscom'] only an idiot would use and believe anything this this republican propaganda machine publishes wwwrealclearpoliticscom\n",
      "believe\n",
      "7\n",
      "7\n",
      "idiot\n",
      "5\n",
      "5\n",
      "machine\n",
      "7\n",
      "7\n",
      "propaganda\n",
      "10\n",
      "10\n",
      "publishes\n",
      "9\n",
      "9\n",
      "republican\n",
      "10\n",
      "10\n",
      "use\n",
      "3\n",
      "3\n",
      "wwwrealclearpoliticscom\n",
      "23\n",
      "23\n",
      "[28, 29, 30, 31, 32, 33, 34, 8, 9, 10, 11, 12, 77, 78, 79, 80, 81, 82, 83, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 85, 86, 87, 88, 89, 90, 91, 92, 93, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 20, 21, 22, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117]\n",
      "Pandas(Index=10, sentence='thanks a lot douchebag', toxicity_sentence=0.0)\n",
      "Pandas(Index=11, sentence='youre the reason the portland buttfuckers are getting pushed out and moving down south to the wilsonville area and beyond', toxicity_sentence=0.0)\n",
      "Pandas(Index=12, sentence='because of you all the stupid fucking asshole that made portland a place full of stupid fucking asshole are going to try to turn salem into some kind of newstupidfuckingassholetopia', toxicity_sentence=0.0)\n",
      "['asshole' 'fucking' 'going' 'kind' 'newstupidfuckingassholetopia' 'place'\n",
      " 'portland' 'salem' 'stupid' 'try' 'turn'] because of you all the stupid fucking asshole that made portland a place full of stupid fucking asshole are going to try to turn salem into some kind of newstupidfuckingassholetopia\n",
      "asshole\n",
      "7\n",
      "7\n",
      "fucking\n",
      "7\n",
      "7\n",
      "going\n",
      "5\n",
      "5\n",
      "kind\n",
      "4\n",
      "4\n",
      "newstupidfuckingassholetopia\n",
      "28\n",
      "28\n",
      "place\n",
      "5\n",
      "5\n",
      "portland\n",
      "8\n",
      "8\n",
      "salem\n",
      "5\n",
      "5\n",
      "stupid\n",
      "6\n",
      "6\n",
      "try\n",
      "3\n",
      "3\n",
      "turn\n",
      "4\n",
      "4\n",
      "[38, 39, 40, 41, 42, 43, 44, 30, 31, 32, 33, 34, 35, 36, 108, 109, 110, 111, 112, 145, 146, 147, 148, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 67, 68, 69, 70, 71, 56, 57, 58, 59, 60, 61, 62, 63, 129, 130, 131, 132, 133, 23, 24, 25, 26, 27, 28, 117, 118, 119, 124, 125, 126, 127]\n",
      "Pandas(Index=13, sentence='this will drive up the price of housing and displace people down here', toxicity_sentence=0.0)\n",
      "Pandas(Index=14, sentence='having lived in san clemente for a time myself i think youre dumb a shit to have moved up here', toxicity_sentence=0.0)\n",
      "Pandas(Index=15, sentence='the fuck wrong with you outsider', toxicity_sentence=0.0)\n",
      "['fuck' 'outsider' 'wrong'] the fuck wrong with you outsider\n",
      "fuck\n",
      "4\n",
      "4\n",
      "outsider\n",
      "8\n",
      "8\n",
      "wrong\n",
      "5\n",
      "5\n",
      "[4, 5, 6, 7, 24, 25, 26, 27, 28, 29, 30, 31, 9, 10, 11, 12, 13]\n",
      "Pandas(Index=16, sentence='i hope bum shit in your yard everyday and never let you walk by without demanding spare change', toxicity_sentence=0.0)\n",
      "Pandas(Index=17, sentence='kick all the non human criminal illegals out of pa and plenty of room fool', toxicity_sentence=0.0)\n",
      "Pandas(Index=18, sentence='because driving under ontario law is stupid enough in ontario', toxicity_sentence=1.0)\n",
      "['driving' 'law' 'ontario' 'stupid'] because driving under ontario law is stupid enough in ontario\n",
      "driving\n",
      "7\n",
      "7\n",
      "law\n",
      "3\n",
      "3\n",
      "ontario\n",
      "7\n",
      "7\n",
      "stupid\n",
      "6\n",
      "6\n",
      "[8, 9, 10, 11, 12, 13, 14, 30, 31, 32, 22, 23, 24, 25, 26, 27, 28, 37, 38, 39, 40, 41, 42]\n",
      "Pandas(Index=19, sentence='youre wrong', toxicity_sentence=0.0)\n"
     ]
    }
   ],
   "source": [
    "samples_to_test = train_df.head(20)\n",
    "for sample in samples_to_test.itertuples():\n",
    "    print(sample)\n",
    "    x = count_vect.transform([sample.sentence])\n",
    "    y = model.predict(x)\n",
    "    if y == 1.0:\n",
    "        toxic = getToxicWordsBayes(count_vect, x, 0.5)\n",
    "        spans = getSpansByToxicWords(toxic,sample.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
